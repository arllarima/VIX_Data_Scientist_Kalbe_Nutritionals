# -*- coding: utf-8 -*-
"""VIX_DS_Kalbe_Nutritionals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18vTk9jk2XFRMePLmSEdAWq-dmJxWWFZ3

# Import Library and Read Dataset
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings("ignore")

# read dataset
customer = pd.read_csv('customer.csv', sep =';')
product = pd.read_csv('product.csv', sep =';')
store = pd.read_csv('store.csv', sep =';')
transaction = pd.read_csv('transaction.csv', sep =';')

customer.head()

product.head()

store.head()

transaction.head()

"""# Data Cleansing

## Check Data Type & Duplicate Data

### Customer
"""

customer.info()

# Check for duplicated rows
print('Number of duplicated rows:', customer.duplicated().sum())

"""- Dataset memiliki 447 baris dan 5 fitur
- Tipe data : object dan int64
- Tidak terdapat data duplikat

**To do:** merubah tipe data income menjadi float
"""

# change data type "income"
customer["Income"] = customer["Income"].str.replace(",", ".").astype(float)

"""### Product"""

product.info()

# Check for duplicated rows
print('Number of duplicated rows:', product.duplicated().sum())

"""- Dataset memiliki 10 baris dan 3 fitur
- Tipe data : object dan int64
- Tidak terdapat data duplikat

### Store
"""

store.info()

# Check for duplicated rows
print('Number of duplicated rows:', store.duplicated().sum())

"""- Dataset memiliki 14 baris dan 6 fitur
- Tipe data : object dan int64
- Tidak terdapat data duplikat

**To do:** merubah tipe data Latitude dan Longtitude menjadi float
"""

# change data type "Latitude" and "Longtitude"
store["Latitude"] = store["Latitude"].str.replace(",", ".").astype(float)
store["Longitude"] = store["Longitude"].str.replace(",", ".").astype(float)

"""### Transaction"""

transaction.info()

# Check for duplicated rows
print('Number of duplicated rows:', transaction.duplicated().sum())

"""- Dataset memiliki 5020 baris dan 8 fitur
- Tipe data : object dan int64
- Tidak terdapat data duplikat

**To do:** merubah tipe data Date menjadi datetime
"""

# change data type column Date to datetime
from datetime import datetime
transaction['Date'] = pd.to_datetime(transaction['Date'],format='%d/%m/%Y')
transaction.head()

"""## Check & Handle Missing Values

### Customer
"""

customer.isna().sum()

"""- Terdapat missing/null value pada fitur Marital Status
- Nilai null pada Marital Status akan di drop, karena kuran dari 10% data
"""

# Handling null values 'Marital Status'
customer.dropna(subset=['Marital Status'], inplace=True)

customer.isna().sum()

"""### Product"""

product.isna().sum()

"""Tidak terdapat missing/null value pada tabel product

### Store
"""

store.isna().sum()

"""Tidak terdapat missing/null value pada tabel store

### Transaction
"""

transaction.isna().sum()

"""Tidak terdapat missing/null value pada tabel transaction

# Merge All Table

Setiap tabel memiliki kolom yang berinisial ID, yaitu CustomerID, ProductID, StoreID, TransactionID.
Disini menggunakan Tabel Transaction untuk menjadi pilar karena pada tabel tersebut memiliki ke-4 kolom berinisial ...ID.
"""

# Merge Customer ke Transaction menjadi df1
df1 = pd.merge(transaction, customer, on=['CustomerID'])

# Merge Store ke df1 menjadi df2
df2 = pd.merge(df1, store, on=['StoreID'])

# Merge Product ke df2 menjadi df
df = pd.merge(df2, product.drop(columns=['Price']), on=['ProductID'])

df.head()

"""# Regression

## Import Library Regression
"""

! pip install pmdarima

from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.stattools import adfuller
from pmdarima import auto_arima
from statsmodels.tsa.arima.model import ARIMA
from pandas.plotting import autocorrelation_plot
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error

"""## Forecasting Data

Menggabungkan semua kolom menjadi Date dan Qty untuk bisa diprediksi time series
"""

df_forecast = df[['Date','Qty']]
df_forecast = df_forecast.groupby('Date')[['Qty']].sum()
df_forecast.head(5)

# Plot the quantity based on date
df_forecast.plot()
plt.show()

decompose = seasonal_decompose(df_forecast)

fig,ax = plt.subplots(3,1,figsize=(15,12))
decompose.trend.plot(ax=ax[0])
ax[0].set_title('Trend')
decompose.seasonal.plot(ax=ax[1])
ax[1].set_title('Seasonal')
decompose.resid.plot(ax=ax[2])
ax[2].set_title('Residual')

plt.tight_layout()
plt.show()

"""## Check Stationarity Data"""

# Plot ACF using default method
plot_acf(df_forecast, lags=30)
plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.title('Autocorrelation Function (ACF)')
plt.show()

# Plot PACF using ywm method
plot_pacf(df_forecast, lags=30,  method='ywm')
plt.xlabel('Lag')
plt.ylabel('Partial Autocorrelation')
plt.title('Partial Autocorrelation Function (PACF)')
plt.show()

# Check if the data stationary or not
df_check_stat = adfuller(df_forecast)
print('ADF Statistic: %f' % df_check_stat[0])
print('P-Value: %f' % df_check_stat[1])
for key, value in df_check_stat[4].items():
  print('Critical Values:')
  print('\t%s: %.3f' % (key, value))

"""Dapat dilihat bahwa setelah dilakukan cek menggunakan adfuller menunjukkan ADF statistics < Critical values dan P value < 0.05 maka data tersebut sudah stationary

## Data Modelling
"""

# Split the data into training and testing sets
cut_off = round(df_forecast.shape[0] * 0.8)
df_train = df_forecast[:cut_off]
df_test = df_forecast[cut_off:]

# Check the shapes of the train and test sets
print("Training set shape:", df_train.shape)
print("Testing set shape:", df_test.shape)

df_train

df_test

# check which ARIMA models best suit using auto ARIMA
auto_arima_model = auto_arima(df_train['Qty'], seasonal=False, stepwise=False, suppress_warnings=True, trace = True)
auto_arima_model.summary()

"""Berdasarkan auto ARIMA model yang terbaik adalah dengan order (2, 0, 2)"""

# Function to calculate RMSE
def rmse(y_actual, y_pred):
  print(f'RMSE value {mean_squared_error(y_actual, y_pred)**0.5}')

# Function to eval machine learning modelling
def eval(y_actual, y_pred):
  print(f'MAE value {mean_absolute_error(y_actual, y_pred)}')

# ARIMA Modelling
y = df_train['Qty']
model = ARIMA(y, order = (2, 0, 2))
model = model.fit()

# Model Prediction
pred = model.get_forecast(len(df_test))
pred_df = pred.conf_int()
pred_df['predictions'] = model.predict(start = pred_df.index[0], end = pred_df.index[-1])
pred_df.index = df_test.index
pred_out = pred_df['predictions']

# Evaluate the model
rmse(df_test['Qty'], pred_out)
eval(df_test['Qty'], pred_out)

# Plot
plt.figure(figsize=(20, 5))
plt.plot(df_train['Qty'], label='Data Train')
plt.plot(df_test['Qty'], color='red', label='Data Test')
plt.plot(pred_out, color='black', label='ARIMA Prediction')
plt.legend()

# Plot and Visualize Predicted Values for Quantity Sold
plt.figure(figsize=(10, 6))
plt.plot(pred_out, color='black', label='ARIMA Prediction')
plt.title('Quantity Sold Forecast')
plt.legend()
plt.show()

# Check Mean Quantity Sold per Day
mean_qty_sold = pred_out.mean().round()
print(f"Mean Quantity Sold Per day: {mean_qty_sold}")

"""Dari perkiraan rata-rata jumlah produk yang terjual pada bulan Januari 2023 adalah 51 produk/hari

## Forecast Each Product Quantity
"""

unique_product = df['Product Name'].unique()
product_pred = pd.DataFrame()

# loop to get forecasts for each product
for product_name in unique_product:
    product_data = df[df['Product Name'] == product_name]

    product_data['Date'] = pd.to_datetime(product_data['Date'])
    product_data.set_index('Date', inplace=True)
    product_data = product_data.resample('D').sum().fillna(0)

    # fit model and get forecasts
    model = ARIMA(product_data['Qty'], order=(2, 0, 2))
    model_fit = model.fit()
    forecast_steps = df_test.shape[0]
    forecast = model_fit.get_forecast(steps=forecast_steps)

    # add forecasted values to dataframe
    forecast_mean = forecast.predicted_mean
    product_pred[product_name] = forecast_mean

product_pred

# Plot forecasting
plt.figure(figsize=(12, 5))
for i in product_pred.columns:
    plt.plot(product_pred.index, product_pred[i], label=i)

plt.legend(loc='center left', bbox_to_anchor=(1, 0.2))
plt.title('Products Quantity Sold Forecasting')
plt.xlabel('Date')
plt.ylabel('Quantity Sold')
plt.show()

# get the average units sold for each product
product_avg_qty = product_pred.mean().round()
print(product_avg_qty)

product_avg_qty.sum()

"""Seperti yang telah dihitung sebelumnya, memasuki tahun 2023, model memperkirakan rata-rata total 51 unit/hari dengan jumlah penjualan harian untuk setiap produk sebagai berikut:<br>

- Crackers : 5 unit
- Yoghurt: 5 unit
- Ginger Candy: 7 unit
- Cheese Stick : 5 unit
- Thai tea : 8 unit
- Choco Bar : 7 unit
- Oat : 3 unit
- Potato Chip : 3 unit
- Coffee Candy : 6 unit
- Cashew : 2 unit <br>

Data di atas dapat digunakan sebagai panduan untuk memenuhi permintaan saat menimbun persediaan. Ke depannya, prediksi ini harus diuji terhadap penjualan aktual dan disesuaikan untuk mengoptimalkan penjualan dan mengurangi pemborosan.

# Clustering

## Import Library Clustering
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn import preprocessing

# read dataset
df.head()

# Identified columns with high correlation
df.corr()

df_cluster = df.groupby(['CustomerID']).agg({
    'TransactionID' : 'count',
    'Qty' : 'sum',
    'TotalAmount' : 'sum'
}).reset_index()

df_cluster

"""## Standarisasi Data"""

from sklearn.preprocessing import StandardScaler

# Scale using StandardScaler
data_cluster = df_cluster.drop(columns=['CustomerID'])
scaler = StandardScaler()
clustering_df_scaled = scaler.fit_transform(data_cluster)

# Convert scaled data to DataFrame
clustering_df_scaled = pd.DataFrame(clustering_df_scaled, columns=data_cluster.columns)

clustering_df_scaled

"""## Elbow Method"""

# create an empty list to store the inertia values
inertia = []

# create a range of k values to test
k_range = range(1, 11)

# fit KMeans for each k value and append the inertia to the list
for k in k_range:
    model = KMeans(n_clusters=k, n_init='auto')
    model.fit(clustering_df_scaled)
    inertia.append(model.inertia_)

# plot the inertia values against k values
plt.plot(k_range, inertia, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

"""Metode elbow digunakan untuk menentukan jumlah cluster yang terbaik yang dapat digunakan untuk menghasilkan hasil cluster yang terbaik dan dapat memaksimalkan kualitas hasil cluster.

Pada grafik, terlihat di titik setelah k = 3 tidak mengalami banyak perubahan, jadi cluster terbaik dibagi menjadi 3 label.

## K-Means Clustering
"""

# instantiate the KMeans model with the selected number of clusters
model = KMeans(n_clusters=3, random_state = 42, n_init='auto')

# fit the model to the scaled data
model.fit(clustering_df_scaled)

# create a new column in the dataframe for the cluster labels
df_cluster['cluster'] = model.labels_

df_cluster

# Convert 'cluster' column to categorical data type
df_cluster['cluster'] = df_cluster['cluster'].astype('category')

# Create the scatter plot using Seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Qty', y='TotalAmount', data=df_cluster, hue='cluster', palette='Set1', s=70)
plt.xlabel('Quantity')
plt.ylabel('TotalAmount')
plt.title('KMeans Clustering Customer Segmentation')
plt.legend(title='Cluster')
plt.show()

df_cluster.groupby(['cluster']).agg({
    'CustomerID': 'count',
    'TransactionID': 'mean',
    'Qty': 'mean',
    'TotalAmount' : 'mean'
})

"""## Interpretasi Data

Berdasarkan hasil clustering, dapat diidentifikasi 3 kelompok pelanggan yang berbeda berdasarkan karakteristik pembelian mereka :

**Cluster 0 (Bronze Buyer)**
*   Jumlah Pelanggan: 135
*   Rata-rata Jumlah Transaksi: 7
*   Rata-rata Jumlah Barang Terjual (Qty): 26
*   Rata-rata Pendapatan (TotalAmount): Rp. 229,388,000

Cluster ini memiliki transaksi, jumlah barang terjual, dan pembelian
**terkecil** dibandingkan cluster lainnya. Kelompok ini cenderung melakukan **pembelian dalam jumlah kecil** dan **frekuensi kecil**. Cluster ini didefinisikan sebagai **Bronze Buyer**.

**Cluster 1 (Silver Buyer)**
*   Jumlah Pelanggan: 202
*   Rata-rata Jumlah Transaksi: 11
*   Rata-rata Jumlah Barang Terjual (Qty): 41
*   Rata-rata Pendapatan (TotalAmount): Rp. 363,267,000

Cluster ini memiliki transaksi, jumlah barang terjual, dan pembelian dalam jumlah **menengah**. Tidak sekecil cluster 0 dan tidak sebesar cluster 2. Cluster ini didefinisikan sebagai **Silver Buyer**.

**Cluster 2 (Gold Buyer)**
*   Jumlah Pelanggan: 107
*   Rata-rata Jumlah Transaksi: 15
*   Rata-rata Jumlah Barang Terjual (Qty): 57
*   Rata-rata Pendapatan (TotalAmount): Rp. 525,431,000

Cluster ini mempunyai rata-rata **transaksi tertinggi**, rata-rata **kuantitas pembelian produk tertinggi**, dan rata-rata **total pembelian terbesar** dibandingkan cluster lainnya. Kelompok ini didefinisikan sebagai **Gold Buyer**.

## Rekomendasi

**Cluster 0 (Bronze Buyer)** <br>
Cluster ini memiliki transaksi, jumlah barang terjual, dan pembelian terkecil dibandingkan cluster lainnya. Oleh karena itu, kami ingin memberikan insentif kepada Cluster 0 agar berbelanja lebih sering, seperti:
*   **Diskon Produk:** Memberi diskon khusus untuk produk-produk tertentu kepada pelanggan dalam kelompok ini. Ini mungkin membantu menarik perhatian mereka untuk melakukan pembelian lebih besar.
*   **Pengiriman Gratis:** Memberi pengiriman gratis untuk pesanan di atas batas tertentu. Ini dapat mendorong pelanggan dalam kelompok ini untuk menambah item ke keranjang belanjaan mereka.
*   **Diskon Kategori Produk:** Memberikan diskon pada kategori-kategori produk yang sering mereka beli. Ini bisa menjadi produk-produk yang lebih terjangkau.

**Cluster 1 (Silver Buyer)** <br>
Cluster ini memiliki transaksi, jumlah barang terjual, dan pembelian dalam jumlah menengah. Oleh karena itu, kami ingin memberikan beberapa penawaran kepada Cluster 1 karena mereka berpotensi bertransisi menjadi cluster 2 (Gold Buyer), seperti:
*   **Cashback:** Memberikan cashback atau voucher diskon untuk transaksi berikutnya jika mereka mencapai batas pembelian tertentu.
*   **Bundling Produk:** Membuat bundle produk dengan harga khusus. Ini dapat mendorong mereka untuk membeli lebih banyak item dalam satu transaksi.
*   **Promo Produk Baru:** Beri tahu mereka tentang produk baru atau edisi terbatas yang akan segera dirilis.

**Cluster 2 (Gold Buyer)** <br>
Cluster ini memiliki transaksi, jumlah barang terjual, dan pembelian terbesar dibandingkan cluster lainnya. Oleh karena itu, kami ingin mempertahankan keterlibatan cluster Gold Buyer ini, atau bahkan lebih. Beberapa kemungkinan strategi untuk mempertahankan cluster ini adalah:
*   **Diskon Besar:** Menawarkan diskon besar untuk produk-produk berkualitas tinggi atau kategori-kategori premium yang mungkin menarik bagi mereka.
*   **Layanan Premium:** Menawarkan layanan pelanggan premium seperti pengiriman ekspres atau akses awal ke penawaran eksklusif.
*   **Program Loyalitas:** Mengajak mereka untuk bergabung dengan program loyalitas yang memberikan poin atau hadiah spesial untuk setiap transaksi.
"""

